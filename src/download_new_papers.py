# encoding: utf-8
import os
import tqdm
from bs4 import BeautifulSoup as bs
import urllib.request
import json
import datetime
import pytz
import re


def _download_new_papers(field_abbr):
    NEW_SUB_URL = f'https://arxiv.org/list/{field_abbr}/new'  # https://arxiv.org/list/cs/new
    page = urllib.request.urlopen(NEW_SUB_URL)
    soup = bs(page)
    content = soup.body.find("div", {'id': 'content'})

    # find the first h3 element in content
    h3 = content.find("h3").text  # e.g: New submissions for Wed, 10 May 23
    date = h3.replace("New submissions for", "").strip()

    dt_list = content.dl.find_all("dt")
    dd_list = content.dl.find_all("dd")
    arxiv_base = "https://arxiv.org/abs/"

    assert len(dt_list) == len(dd_list)
    new_paper_list = []
    for i in tqdm.tqdm(range(len(dt_list))):
        paper = {}

        # æ”¹è¿›çš„è®ºæ–‡ç¼–å·æå–é€»è¾‘
        paper_number = None

        # æ–¹æ³•1: ä»HTMLé“¾æ¥ä¸­æå– (æœ€å¯é )
        link_element = dt_list[i].find("a", href=True)
        if link_element:
            href = link_element.get('href')
            if href and href.startswith('/abs/'):
                paper_number = href[5:]  # å»æ‰ "/abs/" å‰ç¼€
                print(f"ä»é“¾æ¥æå–è®ºæ–‡ç¼–å·: {paper_number}")

        # æ–¹æ³•2: ä»é“¾æ¥æ–‡æœ¬ä¸­æå–
        if not paper_number and link_element:
            link_text = link_element.text.strip()
            # åŒ¹é… arXiv:XXXX.XXXXX æ ¼å¼
            arxiv_match = re.search(r'arXiv:(\d{4}\.\d{4,5})', link_text)
            if arxiv_match:
                paper_number = arxiv_match.group(1)
                print(f"ä»é“¾æ¥æ–‡æœ¬æå–è®ºæ–‡ç¼–å·: {paper_number}")

        # æ–¹æ³•3: ä»æ•´ä¸ªdtå…ƒç´ æ–‡æœ¬ä¸­æå– (å¤‡ç”¨æ–¹æ³•)
        if not paper_number:
            dt_text = dt_list[i].text.strip()
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… arXiv:XXXX.XXXXX æ ¼å¼
            arxiv_match = re.search(r'arXiv:(\d{4}\.\d{4,5})', dt_text)
            if arxiv_match:
                paper_number = arxiv_match.group(1)
                print(f"ä»dtæ–‡æœ¬æå–è®ºæ–‡ç¼–å·: {paper_number}")
            else:
                # æœ€åçš„å¤‡ç”¨æ–¹æ³•ï¼šå°è¯•åŸå§‹çš„åˆ†å‰²é€»è¾‘
                try:
                    parts = dt_text.split()
                    for part in parts:
                        if ':' in part and ('arXiv:' in part or re.match(r'\d{4}\.\d{4,5}', part.split(':')[-1])):
                            paper_number = part.split(":")[-1]
                            print(f"ä»åˆ†å‰²æ–‡æœ¬æå–è®ºæ–‡ç¼–å·: {paper_number}")
                            break
                except:
                    pass

        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ç¼–å·ï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤å€¼å¹¶è®°å½•é”™è¯¯
        if not paper_number:
            print(f"è­¦å‘Š: æ— æ³•æå–ç¬¬ {i + 1} ç¯‡è®ºæ–‡çš„ç¼–å·ï¼Œdtæ–‡æœ¬: {dt_list[i].text.strip()}")
            paper_number = f"unknown_{i}"  # ä¸´æ—¶ç¼–å·ï¼Œé¿å…ç¨‹åºå´©æºƒ

        # æ„å»ºé“¾æ¥
        paper['main_page'] = arxiv_base + paper_number
        paper['pdf'] = arxiv_base.replace('abs', 'pdf') + paper_number

        # æå–å…¶ä»–ä¿¡æ¯
        paper['title'] = dd_list[i].find("div", {"class": "list-title mathjax"}).text.replace("Title: ", "").strip()
        paper['authors'] = dd_list[i].find("div", {"class": "list-authors"}).text \
            .replace("Authors:\n", "").replace("\n", "").strip()
        paper['subjects'] = dd_list[i].find("div", {"class": "list-subjects"}).text.replace("Subjects: ", "").strip()
        paper['abstract'] = dd_list[i].find("p", {"class": "mathjax"}).text.replace("\n", " ").strip()

        new_paper_list.append(paper)

    #  check if ./data exist, if not, create it
    if not os.path.exists("./data"):
        os.makedirs("./data")

    # save new_paper_list to a jsonl file, with each line as the element of a dictionary
    date = datetime.date.fromtimestamp(datetime.datetime.now(tz=pytz.timezone("America/New_York")).timestamp())
    date = date.strftime("%a, %d %b %y")
    with open(f"./data/{field_abbr}_{date}.jsonl", "w") as f:
        for paper in new_paper_list:
            f.write(json.dumps(paper) + "\n")


def get_papers(field_abbr, limit=None):
    date = datetime.date.fromtimestamp(datetime.datetime.now(tz=pytz.timezone("America/New_York")).timestamp())
    date = date.strftime("%a, %d %b %y")
    if not os.path.exists(f"./data/{field_abbr}_{date}.jsonl"):
        _download_new_papers(field_abbr)
    results = []
    with open(f"./data/{field_abbr}_{date}.jsonl", "r") as f:
        for i, line in enumerate(f.readlines()):
            if limit and i == limit:
                return results
            results.append(json.loads(line))
    return results


def test_paper_extraction():
    """
    æµ‹è¯•è®ºæ–‡æå–åŠŸèƒ½çš„è¾…åŠ©å‡½æ•°
    """
    print("ğŸ§ª æµ‹è¯•è®ºæ–‡é“¾æ¥æå–åŠŸèƒ½...")

    # æµ‹è¯•ä¸€ä¸ªç®€å•çš„CSé¢†åŸŸ
    try:
        papers = get_papers("cs", limit=3)
        print(f"âœ… æˆåŠŸæå– {len(papers)} ç¯‡è®ºæ–‡")

        for i, paper in enumerate(papers, 1):
            print(f"\nğŸ“„ è®ºæ–‡ {i}:")
            print(f"  æ ‡é¢˜: {paper.get('title', 'N/A')[:80]}...")
            print(f"  é“¾æ¥: {paper.get('main_page', 'N/A')}")
            print(f"  PDF: {paper.get('pdf', 'N/A')}")

            # éªŒè¯é“¾æ¥æ ¼å¼
            main_page = paper.get('main_page', '')
            if main_page and main_page != 'https://arxiv.org/abs/':
                if re.match(r'https://arxiv\.org/abs/\d{4}\.\d{4,5}', main_page):
                    print(f"  âœ… é“¾æ¥æ ¼å¼æ­£ç¡®")
                else:
                    print(f"  âŒ é“¾æ¥æ ¼å¼å¼‚å¸¸: {main_page}")
            else:
                print(f"  âŒ ç©ºé“¾æ¥æˆ–æ ¼å¼é”™è¯¯")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_paper_extraction()